{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPM Dataset Builder\n",
    "\n",
    "This notebook is designed to retrieve genetic polymorphism data from multiple sources. It uses the LitVar API to extract polymorphisms for each human gene within the LitVar database along with all associated PubMed Identifiers (PMIDs). These PMIDs are then employed as queries on PubMed to obtain MEDLINE data (parsed though the 'nbib' package). All collected data are ultimately consolidated into a single CSV file, known as the \"GRPM Dataset\", which serves as the primary source against which MeSH term queries can be launched to retrieve genes and polymorphisms associated with specific contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Only for Google Colab\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# @markdown Run in Colab virtual machine by default\n",
    "\n",
    "# @markdown to run in google drive set:\n",
    "import_mydrive = False #@param {type:\"boolean\"}\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install nbib\n",
    "    !pip install biopython\n",
    "\n",
    "    if import_mydrive:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        if os.path.exists('/content/drive/MyDrive/grpm_system/'):\n",
    "            %cd /content/drive/MyDrive/grpm_system/\n",
    "        else:\n",
    "            %mkdir /content/drive/MyDrive/grpm_system/\n",
    "            %cd /content/drive/MyDrive/grpm_system/\n",
    "    else:\n",
    "        if os.path.exists('/content/grpm_system/'):\n",
    "            %cd /content/grpm_system/\n",
    "        else:\n",
    "            %mkdir /content/grpm_system/\n",
    "            %cd /content/grpm_system/\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get required data from Zenodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Required datasets from Zenodo Repository\n",
    "#https://zenodo.org/record/8205724  DOI: 10.5281/zenodo.8205724\n",
    "\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "def get_and_extract(file, dir = os.getcwd()):\n",
    "    url = \"https://zenodo.org/record/8205724/files/\"+file+\".zip?download=1\"\n",
    "    zip_file_name = file+\".zip\"\n",
    "    extracted_folder_name = dir\n",
    "\n",
    "    # Download the ZIP file\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Extract the ZIP contents\n",
    "        with io.BytesIO(response.content) as zip_buffer:\n",
    "            with zipfile.ZipFile(zip_buffer, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extracted_folder_name)\n",
    "        print(f\"ZIP file '{zip_file_name}' extracted to '{extracted_folder_name}' successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to download the ZIP file.\")\n",
    "\n",
    "if not os.path.exists('human_geness_repo'):\n",
    "    get_and_extract('human_genes_repo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "def simple_bool(message):\n",
    "    choose = input(message+\" (y/n): \").lower()\n",
    "    your_bool = choose in [\"y\", \"yes\"]\n",
    "    return your_bool\n",
    "\n",
    "def check_and_install_module(module_name):\n",
    "    try:\n",
    "        # Check if the module is already installed\n",
    "        importlib.import_module(module_name)\n",
    "        print(f\"The module '{module_name}' is already installed.\")\n",
    "    except ImportError:\n",
    "        # If the module is not installed, try installing it\n",
    "        x = simple_bool(\n",
    "            \"\\n\" + module_name + \"  module is not installed.\\nwould you like to install it?\")\n",
    "        if x:\n",
    "            import subprocess\n",
    "            subprocess.check_call([\"pip\", \"install\", module_name])\n",
    "            print(f\"The module '{module_name}' was installed correctly.\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "check_and_install_module('nbib')\n",
    "check_and_install_module('requests')\n",
    "\n",
    "#Import Modules\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import nbib\n",
    "import pandas as pd\n",
    "import requests as rq\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "from Bio import Entrez\n",
    "import time\n",
    "\n",
    "Entrez.email = \"your_email@example.com\"\n",
    "\n",
    "request_counter = 0\n",
    "gene_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole genome forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Statistics based on 150 random genes:\n",
    "time_sleep = 0.4\n",
    "runtime_gene = 6.36 #sec/gene\n",
    "genes_hour = 566 #genes/hour\n",
    "request_counter_gene = 4.25 #request/gene (with base sleep (0.4))\n",
    "sleep_request_base = 0.4 #time sleep each request\n",
    "sleep_request_overnight_plus = 1.1 # for an overnight job\n",
    "\n",
    "print('Forecast:')\n",
    "max_genes = int(10000/request_counter_gene)\n",
    "table_size_db_gene = 0.496 #MB\n",
    "table_size_gene = 0.397 #MB\n",
    "png_size_db_gene = 0.47 #KB\n",
    "\n",
    "#Forecast:\n",
    "genes = pd.read_csv('human_genes_repo/H_GENES_proteincoding_genes.csv')\n",
    "ngenes = len(genes)#gene_range\n",
    "nruntime = ngenes * runtime_gene\n",
    "#print('runtime, '+str(ngenes), nruntime)\n",
    "nrequest_counter = ngenes * request_counter_gene\n",
    "\n",
    "tempo_ore = round(nruntime/3600, 2)\n",
    "tempo_ore_overnight = round((nruntime+(sleep_request_overnight_plus*ngenes))/3600, 2)\n",
    "\n",
    "print('max genes/day= ',max_genes)\n",
    "print('for',str(int(ngenes)),'genes:')\n",
    "print('    request counter =', nrequest_counter,'requests')\n",
    "print('    whole genome runtime =', tempo_ore,'hours')\n",
    "print('    whole genome runtime overnight =', tempo_ore_overnight)\n",
    "\n",
    "db_table_size = ngenes * table_size_gene\n",
    "print('    db table size', round(db_table_size,2),'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Human Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Human Gene list ---------------------------------\n",
    "protein_coding_genes = pd.read_csv('human_genes_repo/H_GENES_proteincoding_genes.csv')\n",
    "IG_TR_genes          = pd.read_csv('human_genes_repo/H_GENES_IGTR_genes.csv')\n",
    "RNA_genes            = pd.read_csv('human_genes_repo/H_GENES_RNA_genes.csv')\n",
    "pseudo_genes         = pd.read_csv('human_genes_repo/H_GENES_pseudo_genes.csv')\n",
    "misc_genes           = pd.read_csv('human_genes_repo/H_GENES_misc_genes.csv')\n",
    "\n",
    "# create gene lists:\n",
    "protein_coding_genes_list = protein_coding_genes['Gene name'].dropna().tolist()\n",
    "rna_genes_list = RNA_genes['Gene name'].dropna().tolist()\n",
    "pseudo_genes_list = pseudo_genes['Gene name'].dropna().tolist()\n",
    "\n",
    "\n",
    "# Split job packages:----------------------------------\n",
    "\n",
    "# (1) protein coding genes:\n",
    "gene_range = int(len(protein_coding_genes_list)/18)\n",
    "genes = [protein_coding_genes_list[i * gene_range : (i + 1) * gene_range] for i in range(0, 18)]\n",
    "pcg_chunks = genes[:18]\n",
    "\n",
    "# (2) RNA genes:\n",
    "rna_gene_range = int(len(rna_genes_list)/5)\n",
    "genes = [rna_genes_list[i * rna_gene_range : (i + 1) * rna_gene_range] for i in range(0, 8)]\n",
    "rna_chunks = genes[:5]\n",
    "\n",
    "# (3) pseudo genes:\n",
    "pseudo_gene_range = int(len(pseudo_genes_list)/2)\n",
    "genes = [rna_genes_list[i * pseudo_gene_range : (i + 1) * pseudo_gene_range] for i in range(0, 8)]\n",
    "pseudo_chunks = genes[:2]\n",
    "\n",
    "print('protein_coding_genes',len(protein_coding_genes['Gene name'].dropna()),\n",
    "      '\\nIG_TR_genes',len(IG_TR_genes['Gene name'].dropna()),\n",
    "      '\\nRNA_genes',len(RNA_genes['Gene name'].dropna()),\n",
    "      '\\npseudo_genes',len(pseudo_genes['Gene name'].dropna()),\n",
    "      '\\nmisc_genes',len(misc_genes['Gene name'].dropna()))\n",
    "\n",
    "print('\\nrecommended job lenght for pcg:',int(len(protein_coding_genes_list)/18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set options and import building dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set options:--------------------------------\n",
    "save_plot           = False\n",
    "save_studytype_data = False\n",
    "save_accessory_data = False\n",
    "\n",
    "# set db:--------------------------------\n",
    "db_tag = 'pcg_2024'\n",
    "    # 'pcg'    = protein coding genes = grpm_db\n",
    "    # 'rna'    = rna genes            = grpm_db_rna\n",
    "    # 'pseudo' = pseudogenes          = grpm_db_pseudo\n",
    "\n",
    "db_name = 'grpm_db_'+db_tag\n",
    "db_path = 'grpm_dataset/'+db_name\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    os.makedirs(db_path)\n",
    "#------------------------------------------------\n",
    "\n",
    "# set gene job checkpoint frequency\n",
    "checkpoint = 10\n",
    "\n",
    "#import checkpoint datasets:\n",
    "time_a = datetime.now()\n",
    "if os.path.isfile(db_path+'/grpm_table_output.csv'):\n",
    "    complete_df = pd.read_csv(db_path+'/grpm_table_output.csv',index_col=0)\n",
    "    restart = True\n",
    "else:\n",
    "    complete_df = pd.DataFrame()\n",
    "    restart = False\n",
    "\n",
    "if os.path.isfile(db_path+'/complete_nbibtable.csv'):\n",
    "    complete_nbibtable = pd.read_csv(db_path+'/complete_nbibtable.csv',index_col=0)\n",
    "else:\n",
    "    complete_nbibtable = pd.DataFrame()\n",
    "time_b = datetime.now()\n",
    "\n",
    "print('time load',time_b-time_a)\n",
    "\n",
    "## check saved data:\n",
    "if os.path.isfile(db_path+'/grpm_table_output.csv'):\n",
    "    gene_db_count =  complete_df.gene.nunique()\n",
    "    print('complete_df gene count:',gene_db_count,'on', len(protein_coding_genes_list))\n",
    "    if gene_db_count >= 15519:\n",
    "        print('grpm db already contains all available genes on litvar1')\n",
    "\n",
    "    print('\\ngrpm_table_output.csv size'  ,round(os.path.getsize(db_path+'/grpm_table_output.csv')/(1024*1024),3),'MB')\n",
    "    print('complete_nbibtable.csv size',round(os.path.getsize(db_path+'/complete_nbibtable.csv')/(1024*1024),3),'MB')\n",
    "    print('memory_usage_complete_df'     ,round(complete_df.memory_usage().sum()/(1024*1024),3))\n",
    "    print('memory_usage_complete_nbib_df',round(complete_nbibtable.memory_usage().sum()/(1024*1024),3))\n",
    "else:\n",
    "    print('empty dataset')\n",
    "    print('empty dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set gene-range for this job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the gene list ----------------\n",
    "\n",
    "#1. Set gene-range [whole genome build]\n",
    "    # pcg_chunks    [0:17]\n",
    "    # rna_chunks    [0:4]\n",
    "    # pseudo_chunks [0:1]\n",
    "gene_chunk = pcg_chunks[0]\n",
    "\n",
    "#2. place here your custom gene list [custom build]\n",
    "custom_genes = ['APOA1', 'FFC1', 'ERH', 'USP53']\n",
    "custom_list = simple_bool('Do you want to run the custom list?')\n",
    "\n",
    "#if stucked, store skipped_genes for later:\n",
    "skipped_genes =  []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run a sample?\n",
    "run_sample = simple_bool('Do you want to run a sample?')\n",
    "if run_sample:\n",
    "    sample_size = int(input('sample size? \\nnum:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run Job -------------------------\n",
    "\n",
    "if run_sample:\n",
    "    genes = pd.Series(gene_chunk).sample(sample_size).reset_index(drop=True)\n",
    "    genes = genes.to_list()\n",
    "    restart = False\n",
    "else:\n",
    "    genes = pd.Series(gene_chunk)\n",
    "    genes = genes.to_list()\n",
    "\n",
    "if restart:\n",
    "    restart_from = complete_df.gene.nunique()\n",
    "    gene_start = restart_from\n",
    "    print('search restarted from '+str(restart_from))\n",
    "else:\n",
    "    gene_start = 0\n",
    "\n",
    "if custom_list:\n",
    "    genes = custom_genes\n",
    "    gene_start = 0\n",
    "\n",
    "time_start = datetime.now()\n",
    "print('Start at ',time_start)\n",
    "\n",
    "# get gene data\n",
    "for gene in genes[gene_start:]:\n",
    "\n",
    "    #LitVar2 \"Variants for Gene\" API request\n",
    "    if request_counter > 9950:\n",
    "        print('Request limit reached. Wait \\'till tomorrow!')\n",
    "        pass\n",
    "    time_alpha = datetime.now()\n",
    "    url = \"https://www.ncbi.nlm.nih.gov/research/litvar2-api/variant/search/gene/\" + gene\n",
    "    foo = (rq.get(url)).text\n",
    "\n",
    "    # parsing output in JSON\n",
    "    foo = foo.replace(\"\\n\",\", \")\n",
    "    foo = foo.replace(\"\\'\",\"\\\"\")\n",
    "    foo = foo.replace('\\\"\\\"', '\\\"')\n",
    "    foo = foo.replace('p.\\\"','p.')\n",
    "    foo = foo.replace('c.\\\"','c.')\n",
    "    foo = foo.replace('g.\\\"','g.')\n",
    "    foo = foo.replace('\\\">','>')\n",
    "    foo = foo.replace('.C\\\"204','.C204')  #<= if stucked, look for bugs like this into text\n",
    "\n",
    "    data=\"[\" + foo + \"]\"\n",
    "\n",
    "    #Create Dataframe\n",
    "    #df = pd.read_json(data)\n",
    "    df = pd.read_json(StringIO(data))\n",
    "    if 'rsid' in df.columns and len(df.rsid)>1:\n",
    "        # creare un df senza i clingen.\n",
    "        dfb = df[['_id','pmids_count','rsid']]\n",
    "        dfa = dfb[~dfb['_id'].str.contains('@CA')].drop_duplicates().reset_index(drop=True)\n",
    "        dfn = dfa.dropna(subset=['rsid'])\n",
    "\n",
    "        #Statistics\n",
    "        handle = Entrez.esearch(db=\"snp\", term=gene)\n",
    "        record = Entrez.read(handle)\n",
    "        request_counter += 1\n",
    "\n",
    "        NCBI_dbSNP = record[\"Count\"]\n",
    "        lit2_variant = len(dfa['_id'].drop_duplicates())\n",
    "        lit2_variant_norsid = len(dfa.loc[df['rsid'].isna()])\n",
    "        lit2_rsid = len(dfn.rsid.drop_duplicates())\n",
    "\n",
    "\n",
    "        # remove rs with pmid_count = 1\n",
    "        df2 = dfn.loc[df.pmids_count !=1]#.reset_index(drop=True)\n",
    "        lit2_rsid_f = len(df2)\n",
    "\n",
    "        # accessory data\n",
    "        dfsort = df.sort_values(by='pmids_count',ascending=False).reset_index(drop=True)\n",
    "        df2sort = df2.sort_values(by='pmids_count',ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        #LitVar1 POST rsid2pmids\n",
    "        # convert in \"data\" for POST request\n",
    "        liss = list(map(str, df.rsid))\n",
    "        qrsid = \"\\\", \\\"\".join(liss)\n",
    "        qrsid = '{\"rsids\": [\"'+ qrsid +'\"]}'\n",
    "\n",
    "        url=\"https://www.ncbi.nlm.nih.gov/research/bionlp/litvar/api/v1/public/rsids2pmids\"\n",
    "        headers={ \"Content-Type\" : \"application/json\", \"Accept\" : \"application/json\"}\n",
    "\n",
    "        # enter my data = rsID list\n",
    "        data = qrsid\n",
    "        #data = {[\"rs781028867\", \"rs17817449\"]}\n",
    "        r = rq.post(url, headers=headers, data=data )\n",
    "        if not r.ok:\n",
    "            r.raise_for_status()\n",
    "            sys.exit()\n",
    "        rspost = r.json()\n",
    "\n",
    "        #Display\n",
    "        dfrspost = pd.DataFrame(rspost)\n",
    "        if 'rsid' in dfrspost.columns and len(dfrspost.rsid)>1:\n",
    "            lit1_rsid = len(dfrspost.rsid)\n",
    "            #lit2_rsid = len(df)\n",
    "\n",
    "            lit1_raw_pmid = 0\n",
    "            for i in range(len(dfrspost)):\n",
    "                lit1_raw_pmid += len(dfrspost.pmids[i])\n",
    "\n",
    "            # Creating the simple list [rsid-pmid]\n",
    "            #[MODULE: \"Alligner\"]\n",
    "            rspmid = []\n",
    "            for i in range(len(dfrspost)):\n",
    "                for pmid in dfrspost['pmids'][i]: #dfrspost = mother table\n",
    "                    out = dfrspost['rsid'][i], pmid\n",
    "                    rspmid.append(out)\n",
    "\n",
    "            rsidpmid = pd.DataFrame(rspmid).drop_duplicates().rename(columns={0: 'rsid',1:'pmids'})\n",
    "            rsidpmid['pmids'] = rsidpmid['pmids'].astype(str) \n",
    "            #report data:\n",
    "            lit1_rsid_pmid = len(rsidpmid)\n",
    "            lit1_pmid = len(rsidpmid.drop_duplicates(subset='pmids'))\n",
    "\n",
    "\n",
    "            ####[MODULE: groupby.describe]\n",
    "            # applicare groupby ad rsidpmid per avere tabella pmid count\n",
    "            rsidpmidcount = rsidpmid.groupby('rsid').describe().reset_index()\n",
    "            rsidpmidcount.columns = rsidpmidcount.columns.to_flat_index()\n",
    "\n",
    "            #replace column names\n",
    "            new_column_names = ['rsid', 'pmid_count', 'pmid_unique','pmid_top','pmid_freq']\n",
    "            rsidpmidcount.columns = new_column_names\n",
    "            rsidpmidcountf = rsidpmidcount[['rsid','pmid_unique']]\n",
    "\n",
    "            #report data:\n",
    "            lit1_rsid_f = len(rsidpmidcountf[rsidpmidcountf.pmid_unique!=1])\n",
    "            lit1_rsid_m = len(rsidpmidcountf[rsidpmidcountf.pmid_unique==1])\n",
    "\n",
    "            rsidpmidcountfsort = rsidpmidcountf.sort_values('pmid_unique',ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "            #Filter pmid for rsid with pmid>1\n",
    "            outless = rsidpmidcountfsort[rsidpmidcountfsort.pmid_unique>1]\n",
    "            #creare una mask isin su rsidpmid con outless.rsid\n",
    "            mask = rsidpmid['rsid'].isin(outless.rsid)\n",
    "            rsidpmidless = rsidpmid[mask]\n",
    "            lit1_pmid_f = len(rsidpmidless.pmids.drop_duplicates())\n",
    "\n",
    "\n",
    "            # PubMed query Build:\n",
    "            ### two input alternatives (total LitVar1 and LitVar>1)\n",
    "            #Total\n",
    "            pmid_l = rsidpmid.pmids.drop_duplicates().tolist()\n",
    "            \n",
    "            #Query chunk build (max:1300)\n",
    "            limit = 1300\n",
    "            \n",
    "            ##Define list of queries for PubMed:\n",
    "            query = []\n",
    "            \n",
    "            def query_build(pmid_list):\n",
    "                query = \"+OR+\".join(pmid_list)\n",
    "                return query\n",
    "            \n",
    "            if len(pmid_l)<=limit:\n",
    "                pmid_l01 = pmid_l\n",
    "                query = [query_build(pmid_l01)]\n",
    "            \n",
    "            if limit<len(pmid_l)<=limit*2:\n",
    "                j = len(pmid_l)//2\n",
    "                pmid_l01 = pmid_l[:j]\n",
    "                pmid_l02 = pmid_l[j:]\n",
    "                query = [query_build(pmid_l01),\n",
    "                         query_build(pmid_l02)]\n",
    "            \n",
    "            if limit*2<len(pmid_l)<=limit*3:\n",
    "                j = len(pmid_l)//3\n",
    "                pmid_l01 = pmid_l[:j]\n",
    "                pmid_l02 = pmid_l[j:j*2]\n",
    "                pmid_l03 = pmid_l[j*2:]\n",
    "                query = [query_build(pmid_l01),\n",
    "                         query_build(pmid_l02),\n",
    "                         query_build(pmid_l03)]\n",
    "            \n",
    "            if limit*3<len(pmid_l)<=limit*4:\n",
    "                j = len(pmid_l)//4\n",
    "                pmid_l01 = pmid_l[:j]\n",
    "                pmid_l02 = pmid_l[j:j*2]\n",
    "                pmid_l03 = pmid_l[j*2:j*3]\n",
    "                pmid_l04 = pmid_l[j*3:]\n",
    "                query = [query_build(pmid_l01),\n",
    "                         query_build(pmid_l02),\n",
    "                         query_build(pmid_l03),\n",
    "                         query_build(pmid_l04)]\n",
    "            \n",
    "            if limit*4<len(pmid_l)<=limit*5:\n",
    "                j = len(pmid_l)//5\n",
    "                pmid_l01 = pmid_l[:j]\n",
    "                pmid_l02 = pmid_l[j:j*2]\n",
    "                pmid_l03 = pmid_l[j*2:j*3]\n",
    "                pmid_l04 = pmid_l[j*3:j*4]\n",
    "                pmid_l05 = pmid_l[j*4:]\n",
    "                query = [query_build(pmid_l01),\n",
    "                         query_build(pmid_l02),\n",
    "                         query_build(pmid_l03),\n",
    "                         query_build(pmid_l04),\n",
    "                         query_build(pmid_l05)]\n",
    "            \n",
    "            if limit*5<len(pmid_l)<=limit*6:\n",
    "                j = len(pmid_l)//6\n",
    "                pmid_l01 = pmid_l[:j]\n",
    "                pmid_l02 = pmid_l[j:j*2]\n",
    "                pmid_l03 = pmid_l[j*2:j*3]\n",
    "                pmid_l04 = pmid_l[j*3:j*4]\n",
    "                pmid_l05 = pmid_l[j*4:j*5]\n",
    "                pmid_l06 = pmid_l[j*5:]\n",
    "                query = [query_build(pmid_l01),\n",
    "                         query_build(pmid_l02),\n",
    "                         query_build(pmid_l03),\n",
    "                         query_build(pmid_l04),\n",
    "                         query_build(pmid_l05),\n",
    "                         query_build(pmid_l06)]\n",
    "            \n",
    "            if limit*6<len(pmid_l)<=limit*7:\n",
    "                j = len(pmid_l)//7\n",
    "                pmid_l01 = pmid_l[:j]\n",
    "                pmid_l02 = pmid_l[j:j*2]\n",
    "                pmid_l03 = pmid_l[j*2:j*3]\n",
    "                pmid_l04 = pmid_l[j*3:j*4]\n",
    "                pmid_l05 = pmid_l[j*4:j*5]\n",
    "                pmid_l06 = pmid_l[j*5:j*6]\n",
    "                pmid_l07 = pmid_l[j*6:]\n",
    "                query = [query_build(pmid_l01),\n",
    "                         query_build(pmid_l02),\n",
    "                         query_build(pmid_l03),\n",
    "                         query_build(pmid_l04),\n",
    "                         query_build(pmid_l05),\n",
    "                         query_build(pmid_l06),\n",
    "                         query_build(pmid_l07)]\n",
    "\n",
    "\n",
    "\n",
    "    # Merging requests for the queries\n",
    "            ### carefull: high runtime\n",
    "\n",
    "            time1 = datetime.now()\n",
    "            pages = ((len(pmid_l01)//200)+1)+1\n",
    "            if len(pmid_l01) % 200 == 0:\n",
    "                pages = pages -1\n",
    "            fullnbib = str()\n",
    "            for d in query:\n",
    "                for i in range(1,pages):\n",
    "                    page = str(i)\n",
    "                    url = 'https://pubmed.ncbi.nlm.nih.gov/?term=' + d + '&format=pubmed&size=200&page='+ page\n",
    "                    output = rq.get(url)\n",
    "                    html = output.text\n",
    "                    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "                    for script in soup([\"script\", \"style\"]):\n",
    "                        script.extract()\n",
    "                    text = soup.get_text()\n",
    "                    postString = text.split(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",2)[2]\n",
    "                    nbib01 = postString.replace('\\n\\n','')\n",
    "                    fullnbib += nbib01\n",
    "                    request_counter += pages\n",
    "                    time.sleep(1.5)\n",
    "\n",
    "            time2 = datetime.now()\n",
    "            timestamp = time2.strftime('%Y%m%d%H%M%S')\n",
    "            #^print('finish at:', datetime.now())\n",
    "            runtime = time2-time1\n",
    "            duration = str(runtime).split('.')[0]\n",
    "            hours, minutes, seconds = duration.split(':')\n",
    "            compact_duration = '{}:{}:{}'.format(hours, minutes, seconds)\n",
    "\n",
    "\n",
    "            # nbib parsing:\n",
    "            timea = datetime.now()\n",
    "            ref = nbib.read(fullnbib)\n",
    "            dfbib = pd.DataFrame(ref)\n",
    "            if 'descriptors' in dfbib.columns and len(dfbib['descriptors'])>1:\n",
    "                dfbibdes = dfbib[['pubmed_id','descriptors']].dropna().reset_index(drop=True)\n",
    "                nbib_objects = len(dfbib)\n",
    "                nbib_objects_withdescriptors = len(dfbibdes)\n",
    "                #print('nibib objects:',nbib_objects)\n",
    "                #print('nibib objects with descriptors:',len(dfbibdes))\n",
    "                timeb = datetime.now()\n",
    "                #print('runtime:', timeb-timea)\n",
    "\n",
    "                #Statistics:\n",
    "                pubmed_pmid_query = len(pmid_l)\n",
    "                pubmed_pmid_nbib = len(dfbib.pubmed_id.drop_duplicates())\n",
    "                pubmed_pmid_nbib_yesmesh = len(dfbibdes.pubmed_id.drop_duplicates())\n",
    "                pubmed_pmid_nbib_nomesh = len(dfbib.pubmed_id.drop_duplicates())-len(dfbibdes.pubmed_id.drop_duplicates())\n",
    "\n",
    "                # refine MESH\n",
    "                dfr = []\n",
    "                for i in range(len(dfbibdes)):\n",
    "                    for mesh in dfbibdes['descriptors'][i]:\n",
    "                        out = dfbibdes['pubmed_id'][i], mesh\n",
    "                        dfr.append(out)\n",
    "                MESH = pd.DataFrame(dfr).rename(columns={0: 'pmids',1:'mesh'})\n",
    "\n",
    "                # dataframe parsing splitting three fields\n",
    "                MESHsplit =[]\n",
    "                for i in range(len(MESH)):\n",
    "                    mg = MESH.mesh[i].get('descriptor')\n",
    "                    mg2 = MESH.mesh[i].get('qualifier')\n",
    "                    mg3 = MESH.mesh[i].get('major')\n",
    "                    mgg = MESH.pmids[i], mg, mg2, mg3\n",
    "                    MESHsplit.append(mgg)\n",
    "\n",
    "                dfmesh = pd.DataFrame(MESHsplit).rename(columns={0: 'pmids',1:'mesh',2:'qualifier',3:'major'}).drop_duplicates()\n",
    "\n",
    "                #statistics\n",
    "                pubmed_pmidmesh = len(dfmesh[['pmids','mesh']].drop_duplicates())\n",
    "                pubmed_mesh_qualifier_major = len(MESH.mesh.drop_duplicates())\n",
    "                pubmed_mesh = len(dfmesh.mesh.drop_duplicates())\n",
    "\n",
    "                pmidmesh = dfmesh[['pmids','mesh']].drop_duplicates()\n",
    "                pmidmesh['pmids'] = pmidmesh['pmids'].astype(str) #convert pmid type in str\n",
    "\n",
    "\n",
    "                #Analyze enrichment with groupby.describe method-------------------------------\n",
    "                #Add rsid coulmn con merge\n",
    "                rspmidmesh_merge = pd.merge(pmidmesh, rsidpmid, on= 'pmids', how='inner').drop_duplicates().reindex(columns=['pmids', 'rsid', 'mesh'])\n",
    "                #rspmidmesh_merge['pmids'] = rspmidmesh_merge['pmids'].astype(str)\n",
    "\n",
    "                ### groupby.describe analysis by mesh\n",
    "                meshrspmidmerge_count = rspmidmesh_merge.groupby('mesh').describe().reset_index()\n",
    "                meshrspmidmerge_count.columns = meshrspmidmerge_count.columns.to_flat_index()\n",
    "                #to handle generate df.groupby.describe, convert Multicolumn to single column\n",
    "                #https://datascientyst.com/flatten-multiindex-in-pandas/\n",
    "                new_column_names = ['mesh', 'pmid-count', 'pmid-unique','pmid-top','pmid-freq','rsid-count', 'rsid-unique','rsid-top','rsid-freq']\n",
    "                meshrspmidmerge_count.columns = new_column_names\n",
    "\n",
    "                meshrspmidmerge_count_short = meshrspmidmerge_count[['mesh','pmid-unique','rsid-unique']]\n",
    "                #pmidmeshintmerge2meshlesssort = pmidmeshintmerge2meshless.sort_values(by='pmid-unique',ascending=False).reset_index(drop=True)\n",
    "\n",
    "                # add frequency\n",
    "                totalpmid_count = len(pmidmesh.pmids.drop_duplicates())\n",
    "                meshrspmidmerge_count_short_freq = meshrspmidmerge_count_short.copy()\n",
    "                meshb_frq = meshrspmidmerge_count_short_freq.loc[:,'pmid-unique'].astype(float)/totalpmid_count\n",
    "                meshrspmidmerge_count_short_freq.loc[:,'mesh frequency'] = round(meshb_frq,3)#*100\n",
    "                meshrspmidmerge_count_short_freq_sort = meshrspmidmerge_count_short_freq.sort_values(by='pmid-unique',ascending=False).reset_index(drop=True)\n",
    "\n",
    "                top10mesh_all = meshrspmidmerge_count_short_freq_sort['mesh'][:10].tolist()\n",
    "                #display(meshrspmidmerge_count_short_freq_sort.head(20))\n",
    "\n",
    "                ### groupby.describe analysis by rsid------------------\n",
    "                rspmidmeshmerge_count = rspmidmesh_merge.groupby('rsid').describe().reset_index()\n",
    "                rspmidmeshmerge_count.columns = rspmidmeshmerge_count.columns.to_flat_index()\n",
    "                new_column_names = ['rsid', 'pmid-count', 'pmid-unique','pmid-top','pmid-freq','mesh-count', 'mesh-unique','mesh-top','mesh-freq']\n",
    "                rspmidmeshmerge_count.columns = new_column_names\n",
    "\n",
    "                rsid_pmid10 = len(rspmidmeshmerge_count[rspmidmeshmerge_count['pmid-unique']>10])\n",
    "                rsid_pmid50 = len(rspmidmeshmerge_count[rspmidmeshmerge_count['pmid-unique']>50])\n",
    "                rsid_pmid100 = len(rspmidmeshmerge_count[rspmidmeshmerge_count['pmid-unique']>100])\n",
    "\n",
    "                rspmidmeshmerge_count_short = rspmidmeshmerge_count[['rsid','pmid-unique','mesh-unique']]\n",
    "                rspmidmeshmerge_count_short_sort = rspmidmeshmerge_count_short.sort_values(by='pmid-unique', ascending= False).reset_index(drop=True)\n",
    "                top10rsid_all = rspmidmeshmerge_count_short_sort['rsid'].iloc[:10].tolist()\n",
    "\n",
    "                if save_plot:\n",
    "                    # create a scatter plot-----------------------------------------\n",
    "                    x1 = meshrspmidmerge_count_short_freq_sort['mesh'].head(30)\n",
    "                    y1 = meshrspmidmerge_count_short_freq_sort['pmid-unique'].head(30)\n",
    "                    plt.figure(figsize=(5, 8))\n",
    "                    plt.title('Scatter Plot: '+gene+' pmid-mesh (total)', loc='center',pad=10)\n",
    "                    plt.scatter(y1, x1)\n",
    "                    plt.gca().invert_yaxis()\n",
    "                    #plt.yticks(rotation=90)\n",
    "                    plt.tick_params(axis='x', which='both', top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "                    plt.xlabel('pmid count', position=(0.5, 1.08))\n",
    "                    ax = plt.gca()\n",
    "                    ax.xaxis.set_label_position('top')\n",
    "                    plt.savefig(db_path+'/'+gene+'_mesh_plot_'+timestamp+'_total.png',dpi=120, bbox_inches = \"tight\")\n",
    "                    plt.close()\n",
    "\n",
    "                if save_studytype_data:\n",
    "                    # GET STUDY TYPE from NBIB----------------------------------------\n",
    "                    dfbib = pd.DataFrame(ref)\n",
    "                    dfbib.pubmed_id = dfbib.pubmed_id.astype('str')\n",
    "                    if 'publication_types' in dfbib.columns and len(dfbib['publication_types'])>1:\n",
    "                        dfbib_studyty = dfbib[['pubmed_id','publication_types']].dropna().reset_index(drop=True)\n",
    "\n",
    "                        #PMID-Studytype table build:\n",
    "                        df_studytype = []\n",
    "                        for i in range(len(dfbib_studyty)):\n",
    "                            for studytype in dfbib_studyty['publication_types'][i]:\n",
    "                                out = dfbib_studyty['pubmed_id'][i], studytype\n",
    "                                df_studytype.append(out)\n",
    "                        STUDYT = pd.DataFrame(df_studytype).rename(columns={0: 'pmids',1:'study_type'})\n",
    "                        mask_st = STUDYT['study_type'].str.contains('Research Support|Journal Article')\n",
    "                        STUDYTless = STUDYT[~mask_st].reset_index(drop=True)\n",
    "\n",
    "                        mask_lessing = STUDYT['pmids'].isin(STUDYTless['pmids'])\n",
    "                        STUDYTdiff = STUDYT[~mask_lessing].reset_index(drop=True)\n",
    "                        STUDYTdiff['study_type2'] = 'Unknown'\n",
    "                        STUDYTdiff = STUDYTdiff[['pmids','study_type2']].rename(columns={'study_type2':'study_type'}).drop_duplicates().reset_index(drop=True)\n",
    "                        #len(STUDYTless.pmids.drop_duplicates()), len(dfbib.pubmed_id.drop_duplicates())\n",
    "                        STUDYTconcat = pd.concat([STUDYTless, STUDYTdiff], ignore_index=True)\n",
    "                        STUDYTconcat#.pmids.drop_duplicates()\n",
    "\n",
    "                        #study type count:\n",
    "                        STUDYTless_count = STUDYTconcat.groupby('study_type').describe().reset_index()\n",
    "                        STUDYTless_count.columns = STUDYTless_count.columns.to_flat_index()\n",
    "                        new_column_names = ['study_type', 'pmid-count', 'pmid-unique','pmid-top','pmid-freq']\n",
    "                        STUDYTless_count.columns = new_column_names\n",
    "                        STUDYTless_count_sort = STUDYTless_count.sort_values(by= 'pmid-count',ascending=False)\n",
    "\n",
    "                        # save data\n",
    "                        STUDYTconcat.to_csv(db_path+'/'+gene+'_lit1pmid_studytype.csv')\n",
    "                    else:\n",
    "                        print(gene+' no publication_types in nbib')\n",
    "                        pass\n",
    "\n",
    "#SAVE TABLES-----------------------------------------------------------\n",
    "                timestamp = time2.strftime('%Y%m%d%H%M%S')\n",
    "                # save accessory data:\n",
    "                if save_accessory_data:\n",
    "                    dfsort[[\"_id\",\"rsid\",\"pmids_count\"]].to_csv(db_path+'/'+gene+'_litvar2_variants4gene.csv')\n",
    "                    rsidpmid.to_csv(db_path+'/'+gene+'_litvar1_rsids2pmids.csv') #lit1 [rsid-pmid]\n",
    "                    #rsidpmidcountfsort #lit1 pmid count\n",
    "\n",
    "                    meshrspmidmerge_count_short_freq_sort.to_csv(db_path+'/'+gene+'_mesh_pmidrsid_count.csv')\n",
    "\n",
    "                #complete_df with concat:\n",
    "                #import gene-rsidpmidmesh and gene-rsidpmid\n",
    "                dfmesh['pmids'] = dfmesh['pmids'].astype(str)\n",
    "                rsidpmid['pmids'] = rsidpmid['pmids'].astype(str)\n",
    "\n",
    "                # add a rsid-merger to dfmesh\n",
    "                gene_rsidpmidmesh = pd.merge(rsidpmid, dfmesh, on='pmids')\n",
    "                gene_rsidpmidmesh['gene'] = gene\n",
    "\n",
    "                gene_df = pd.DataFrame(gene_rsidpmidmesh)\n",
    "                complete_df = pd.concat([complete_df, gene_rsidpmidmesh])\n",
    "\n",
    "                #complete_nbibtable with concat:\n",
    "                dfbib['gene'] = gene\n",
    "                complete_nbibtable = pd.concat([complete_nbibtable, dfbib])\n",
    "                #pyperclip.copy(str(dfbib.columns.to_list()))\n",
    "\n",
    "                # save checkpoint----------------------\n",
    "                if genes.index(gene) > 1 and genes.index(gene) % checkpoint == 0:\n",
    "                    complete_df = complete_df.reindex(columns=['gene','rsid', 'pmids', 'mesh', 'qualifier', 'major'])\n",
    "                    complete_df.to_csv(db_path+'/grpm_table_output.csv')\n",
    "\n",
    "                    complete_nbibtable = complete_nbibtable.reindex(columns=['gene','pubmed_id', 'citation_owner', 'nlm_status', 'last_revision_date', 'electronic_issn', 'linking_issn', 'journal_volume', 'journal_issue', 'publication_date', 'title', 'abstract', 'authors', 'language', 'grants', 'publication_types', 'electronic_publication_date', 'place_of_publication', 'journal_abbreviated', 'journal', 'nlm_journal_id', 'descriptors', 'pmcid', 'keywords', 'conflict_of_interest', 'received_time', 'revised_time', 'accepted_time', 'pubmed_time', 'medline_time', 'entrez_time', 'pii', 'doi', 'publication_status', 'print_issn', 'pages'])\n",
    "                    complete_nbibtable.to_csv(db_path+'/complete_nbibtable.csv')\n",
    "                    print(\"saved checkpoint\")\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "\n",
    "#REPORT-----------------------------------------------------------------\n",
    "                time_omega = datetime.now()\n",
    "                full_runtime = time_omega - time_alpha\n",
    "                #^print('total runtime:', full_runtime)\n",
    "                print(gene + '_runtime:', full_runtime)\n",
    "                nbib_seconds = runtime.total_seconds()\n",
    "                total_seconds = full_runtime.total_seconds()\n",
    "                full_runtime_str = str(full_runtime).split('.')[0]\n",
    "\n",
    "                report = {'ncbi_dbsnp': NCBI_dbSNP,\n",
    "                          'lit2_variant': lit2_variant,\n",
    "                          'lit2_variant_norsid': lit2_variant_norsid,\n",
    "                          'lit2_rsid': lit2_rsid,\n",
    "                          'lit2_rsid_plus1': lit2_rsid_f,\n",
    "                          'lit1_rsid': lit1_rsid,\n",
    "                          #'lit1_raw_pmid': lit1_raw_pmid,\n",
    "                          #'lit1_rsid_pmid': lit1_rsid_pmid,\n",
    "                          'lit1_rsid_pmid_plus1': lit1_rsid_f,\n",
    "                          #lit1_rsid_pmid=1': lit1_rsid_m,\n",
    "                          'lit1_pmid': lit1_pmid,\n",
    "                          'lit1_pmid_pmid_plus1': lit1_pmid_f,\n",
    "                          'pubmed_pmid_query': pubmed_pmid_query,\n",
    "                          'nbib_objects': nbib_objects,\n",
    "                          'nbib_objects_withdescriptors': nbib_objects_withdescriptors,\n",
    "                          'pubmed_pmid': pubmed_pmid_nbib,\n",
    "                          'pubmed_pmid_withmesh': pubmed_pmid_nbib_yesmesh,\n",
    "                          #'pubmed_pmid_nomesh':pubmed_pmid_nbib_nomesh,\n",
    "                          'pubmed_pmidmesh': pubmed_pmidmesh,\n",
    "                          'pubmed_mesh_qualifier_major': pubmed_mesh_qualifier_major,\n",
    "                          'pubmed_mesh': pubmed_mesh,\n",
    "                          'rsid_pmid10': rsid_pmid10,\n",
    "                          'rsid_pmid50': rsid_pmid50,\n",
    "                          'rsid_pmid100': rsid_pmid100,\n",
    "                          'top10mesh_all': str(top10mesh_all),\n",
    "                          'top10rsid_all': str(top10rsid_all),\n",
    "                          'pubmed_runtime': duration,\n",
    "                          'total_runtime': full_runtime_str,\n",
    "                          'time_stamp': time2\n",
    "                          }\n",
    "\n",
    "                df_report = pd.DataFrame(report, index=[gene]).transpose()\n",
    "\n",
    "                # generate fist report.csv\n",
    "                if os.path.isfile(db_path+'/GRPM_report.csv'):\n",
    "                    dfL = pd.read_csv(db_path+'/GRPM_report.csv', index_col=0)\n",
    "                    dfL = pd.concat([dfL, df_report], axis=1)\n",
    "                    dfL.to_csv(db_path+'/GRPM_report.csv')\n",
    "                else:\n",
    "                    df_report.to_csv(db_path+'/GRPM_report.csv')  # solo la prima volta\n",
    "\n",
    "                #Update gene values\n",
    "                GRPM_report = pd.read_csv(db_path+'/GRPM_report.csv', index_col=0)\n",
    "                if gene + '.1' in GRPM_report.columns:\n",
    "                    GRPM_report = GRPM_report.drop(columns=gene)\n",
    "                    GRPM_report = GRPM_report.rename(columns={gene + '.1': gene})\n",
    "                    GRPM_report.to_csv(db_path+'/GRPM_report.csv')\n",
    "                    print(gene,'already in db')\n",
    "\n",
    "            else:\n",
    "                print(gene + ' no descriptors in nbib')\n",
    "                time.sleep(0.8)\n",
    "                pass\n",
    "        else:\n",
    "            print(gene + ' no results on litvar1')\n",
    "            time.sleep(0.8)\n",
    "            pass\n",
    "    else:\n",
    "        print(gene + ' no results on litvar2')\n",
    "        pass\n",
    "\n",
    "    if request_counter > 9000:\n",
    "        dada = 2\n",
    "        #print('Allert! Reaching pubmed request limit')\n",
    "    if request_counter > 9950:\n",
    "        #print('Request limit reached. Wait \\'till tomorrow!')\n",
    "        time_finish = datetime.now()\n",
    "        time_batch = time_finish - time_start\n",
    "        time_batch_str = str(time_batch).split('.')[0]\n",
    "        #print('time batch:', time_batch_str)\n",
    "        #break\n",
    "\n",
    "complete_df = complete_df.reindex(columns=['gene','rsid', 'pmids', 'mesh', 'qualifier', 'major'])\n",
    "complete_df.to_csv(db_path+'/grpm_table_output.csv')\n",
    "\n",
    "complete_nbibtable = complete_nbibtable.reindex(columns=['gene','pubmed_id', 'citation_owner', 'nlm_status', 'last_revision_date', 'electronic_issn', 'linking_issn', 'journal_volume', 'journal_issue', 'publication_date', 'title', 'abstract', 'authors', 'language', 'grants', 'publication_types', 'electronic_publication_date', 'place_of_publication', 'journal_abbreviated', 'journal', 'nlm_journal_id', 'descriptors', 'pmcid', 'keywords', 'conflict_of_interest', 'received_time', 'revised_time', 'accepted_time', 'pubmed_time', 'medline_time', 'entrez_time', 'pii', 'doi', 'publication_status', 'print_issn', 'pages'])\n",
    "complete_nbibtable.to_csv(db_path+'/complete_nbibtable.csv')\n",
    "\n",
    "time_finish = datetime.now()\n",
    "time_batch = time_finish - time_start\n",
    "time_batch_str = str(time_batch).split('.')[0]\n",
    "print('gene batch:', len(genes))\n",
    "print('time batch:', time_batch_str)\n",
    "print('runtime/gene:', time_batch/len(genes))\n",
    "print('request_counter:', request_counter,' (limit: 10.000/day)')\n",
    "gene_counter += len(genes)\n",
    "print('requests/gene:', request_counter/gene_counter)\n",
    "print(time_finish)\n",
    "\n",
    "### notes:\n",
    "# LIMITS PubMed Programming Utilities (PMU)\n",
    "# 10 requests/second\n",
    "# 10,000 requests/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('grpm report:')\n",
    "display(pd.read_csv(db_path+'/GRPM_report.csv', index_col=0).T)\n",
    "print('grpm table genes:', len(pd.read_csv(db_path+'/grpm_table_output.csv').gene.drop_duplicates()))\n",
    "\n",
    "print('\\nnbib table:')\n",
    "display(pd.read_csv(db_path+'/complete_nbibtable.csv',index_col=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show saved report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize GRPM_report.csv\n",
    "GRPM_report = pd.read_csv(db_path+'/GRPM_report.csv', index_col=0).transpose().reset_index().rename(columns={'index':'gene'})\n",
    "\n",
    "repo_int_cols = ['ncbi_dbsnp', 'lit2_variant', 'lit2_variant_norsid','lit2_rsid','lit2_rsid_plus1', 'lit1_rsid', 'lit1_rsid_pmid_plus1','lit1_pmid', 'lit1_pmid_pmid_plus1','pubmed_pmid_query',    'nbib_objects', 'nbib_objects_withdescriptors', 'pubmed_pmid', 'pubmed_pmid_withmesh', 'pubmed_pmidmesh','pubmed_mesh_qualifier_major','pubmed_mesh', 'rsid_pmid10','rsid_pmid50', 'rsid_pmid100' ]\n",
    "\n",
    "GRPM_report[repo_int_cols] = GRPM_report[repo_int_cols].astype(int)\n",
    "\n",
    "#display(GRPM_report_less.sort_values(by= 'matching_pmids',ascending=False))\n",
    "GRPM_report.sort_values(by='lit1_pmid',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show Bar Graph\n",
    "GRPM_report_sort = GRPM_report.sort_values(by= 'pubmed_pmid',ascending=False)\n",
    "\n",
    "x = GRPM_report_sort.gene.iloc[:40]\n",
    "y = GRPM_report_sort['pubmed_pmid'].iloc[:40]\n",
    "plt.figure(figsize=(4, 8))\n",
    "plt.title('PMIDs in Dataset', loc='center',pad=10)\n",
    "\n",
    "plt.barh(x,y)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tick_params(axis='x', which='both', top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "#plt.xlabel('pmid count', position=(0.5, 1.08))\n",
    "plt.ylabel('genes')\n",
    "plt.xlabel('pmid count', position=(0.5, 1.08))\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_label_position('top')\n",
    "#plt.savefig('PMID_filtered.png',dpi=300, bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging Code Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debugging: litvar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if stucked check 'data' variable:\n",
    "# replace manually malformed lines\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debugging: nbib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NBIB PROBLEM SOLVER---------------------------------\n",
    "# replace malformed lines\n",
    "fullnbib= fullnbib.replace('2007/09/31','2007/09/30') # <= some dates are mispelled in pubmed\n",
    "ref = nbib.read(fullnbib)\n",
    "dfbib = pd.DataFrame(ref)\n",
    "dfbib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NBIB PROBLEM SOLVER [History] ---------------------------------\n",
    "with open('nbib report '+gene+'.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(fullnbib)\n",
    "with open('nbib report '+gene+'_FIXED.txt', 'r', encoding='utf-8') as file:\n",
    "    fullnbib = file.read() # --> not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eutils: get study type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_and_install_module('biopython')\n",
    "from Bio import Entrez\n",
    "Entrez.email = \"your_email@example.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### EUTILS GET STUDY TYPE MODULE\n",
    "#https://biopython.org/docs/1.76/api/Bio.Entrez.html\n",
    "def get_study_type(pmids):\n",
    "    Entrez.email = 'your_email@your_domain.com'\n",
    "    handle = Entrez.esummary(db='pubmed', id=','.join(pmids), retmode='xml')\n",
    "    records = Entrez.parse(handle)\n",
    "    study_types = []\n",
    "    for record in records:\n",
    "        article_types = record['PubTypeList']\n",
    "        if 'Randomized Controlled Trial' in article_types:\n",
    "            study_types.append('Randomized Controlled Trial')\n",
    "        elif 'Controlled Clinical Trial' in article_types:\n",
    "            study_types.append('Controlled Clinical Trial')\n",
    "        elif 'Cohort Studies' in article_types:\n",
    "            study_types.append('Cohort Study')\n",
    "        elif 'Case-Control Studies' in article_types:\n",
    "            study_types.append('Case-Control Study')\n",
    "        elif 'Review' in article_types:\n",
    "            study_types.append('Review')\n",
    "        elif 'Clinical Trial' in article_types:\n",
    "            study_types.append('Clinical Trial')\n",
    "        elif 'Meta-Analysis' in article_types:\n",
    "            study_types.append('Meta-Analysis')\n",
    "        elif 'Multicenter Study' in article_types:\n",
    "            study_types.append('Multicenter Study')\n",
    "        else:\n",
    "            study_types.append('Unknown')\n",
    "    return study_types\n",
    "\n",
    "pmidlist = list(pmidmesh['pmids'].drop_duplicates())\n",
    "genepmids_str = list(map(str, pmidlist))\n",
    "study_type = get_study_type(genepmids_str)\n",
    "pmids_studytype = pd.DataFrame(list(zip(genepmids_str, study_type)), columns=[gene + '_PMID', 'study type'])\n",
    "request_counter += 1\n",
    "\n",
    "#study type count:\n",
    "pmids_studytype_count = pmids_studytype.groupby('study type').describe().reset_index()\n",
    "pmids_studytype_count.columns = pmids_studytype_count.columns.to_flat_index()\n",
    "new_column_names = ['study_type', 'pmid-count', 'pmid-unique', 'pmid-top', 'pmid-freq']\n",
    "pmids_studytype_count.columns = new_column_names\n",
    "pmids_studytype_countsort = pmids_studytype_count.sort_values(by='pmid-count', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
