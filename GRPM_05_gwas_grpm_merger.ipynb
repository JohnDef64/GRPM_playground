{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GWAS-GRPM Merger"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Only for Google Colab\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# @markdown Run in Colab virtual machine by default\n",
    "\n",
    "# @markdown to run in google drive set:\n",
    "import_mydrive = False #@param {type:\"boolean\"}\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    if import_mydrive:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        if os.path.exists('/content/drive/MyDrive/grpm_system/'):\n",
    "            %cd /content/drive/MyDrive/grpm_system/\n",
    "        else:\n",
    "            %mkdir /content/drive/MyDrive/grpm_system/\n",
    "            %cd /content/drive/MyDrive/grpm_system/\n",
    "    else:\n",
    "        if os.path.exists('/content/grpm_system/'):\n",
    "            %cd /content/grpm_system/\n",
    "        else:\n",
    "            %mkdir /content/grpm_system/\n",
    "            %cd /content/grpm_system/\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory:\", current_directory)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Modules\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def simple_bool(message):\n",
    "    choose = input(message+\" (y/n): \").lower()\n",
    "    your_bool = choose in [\"y\", \"yes\",\"yea\",\"sure\"]\n",
    "    return your_bool\n",
    "\n",
    "def get_file(url, file_name, dir = os.getcwd()):\n",
    "    url = url\n",
    "    file_name = file_name\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        content = response.content\n",
    "        file_path = os.path.join(dir, file_name)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(content)\n",
    "\n",
    "def check_and_install_module(module_name):\n",
    "    try:\n",
    "        # Check if the module is already installed\n",
    "        importlib.import_module(module_name)\n",
    "        print(f\"The module '{module_name}' is already installed.\")\n",
    "    except ImportError:\n",
    "        # If the module is not installed, try installing it\n",
    "        x = simple_bool(\n",
    "            \"\\n\" + module_name + \"  module is not installed.\\nwould you like to install it?\")\n",
    "        if x:\n",
    "            import subprocess\n",
    "            subprocess.check_call([\"pip\", \"install\", module_name])\n",
    "            print(f\"The module '{module_name}' was installed correctly.\")\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get NLTK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get Natural Language Toolkit https://www.nltk.org/\n",
    "check_and_install_module('nltk')\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get pychatgpt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get & import pychatgpt (openai based module)\n",
    "if simple_bool('Do you have an openai API-key?'):\n",
    "    # Get pychatgpt at: https://github.com/johndef64/pychatgpt.git\n",
    "    get_file(url=\"https://raw.githubusercontent.com/johndef64/pychatgpt/main/pychatgpt.py\", file_name='pychatgpt.py')\n",
    "\n",
    "    import pychatgpt as op\n",
    "    # Example usage\n",
    "    message = \"Tell me about GWAS-Catalog\"\n",
    "    response = op.send_message_gpt(message)\n",
    "\n",
    "else:\n",
    "    print('get your api-key at https://platform.openai.com/account/api-keys\\n'\n",
    "          'or simply use web playground at https://platform.openai.com/playground?model=gpt-3.5-turbo-16k')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get requirements"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get MESH.csv from 'bioportal.bioontology.org'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get GWAS dataset at https://www.ebi.ac.uk/gwas/docs/file-downloads\n",
    "\n",
    "if not os.path.exists('gwas_catalog_data'):\n",
    "    os.makedirs('gwas_catalog_data')\n",
    "\n",
    "if not os.path.exists('gwas_catalog_data/gwas_catalog_v1.0.2-associations_e109_r2023-03-27.tsv'):\n",
    "    get_file( url='https://www.ebi.ac.uk/gwas/api/search/downloads/alternative', file_name='gwas_catalog_v1.0.2-associations.tsv', dir = 'gwas_catalog_data')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Workflow:\n",
    "\n",
    "1. Clean GWAS dataset\n",
    "   (in STRONGEST SNP-RISK ALLEL, drop \"?\")\n",
    "2. retrieve GRPM Survey data\n",
    "3. apply GI cut-off (0.0125) on GRPM Survey\n",
    "4. merge GWAS and GRPM on rsIDs\n",
    "5. align GRPM-MESH vs GWAS-mapped-trait\n",
    "6.  creating corrispondence dictionary\n",
    "    ['PUBMED_MESH','DISEASE/TRAIT']\n",
    "    through Tokenization -> Natural Language Toolkit https://www.nltk.org/\n",
    "7. get the STRONGEST SNP-RISK ALLELE\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Import GWAS dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#import dataset\n",
    "# Download gwas_catalog_v1.0.2-associations_e109_r2023-03-27.tsv from: https://www.ebi.ac.uk/gwas/docs/file-downloads\n",
    "\n",
    "df_gwas = pd.read_table('gwas_catalog_data/gwas_catalog_v1.0.2-associations.tsv', low_memory=False)\n",
    "df_gwas[['PUBMEDID','SNP_ID_CURRENT']] = df_gwas[['PUBMEDID','SNP_ID_CURRENT']].astype(str)\n",
    "\n",
    "df_gwas['MAPPED_GENE'] = df_gwas['MAPPED_GENE'].astype(str)  # Convert MAPPED_GENE column to string type\n",
    "clean_df_gwas = df_gwas[~df_gwas['MAPPED_GENE'].str.contains('- |,')] # drop readthough transcripts\n",
    "\n",
    "print('genes: ', clean_df_gwas['MAPPED_GENE'].nunique())\n",
    "print('studies: ', len(df_gwas['STUDY'].drop_duplicates()))\n",
    "print('rsid: ', len(df_gwas['SNP_ID_CURRENT'].drop_duplicates()))\n",
    "df_gwas.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#display selected columns\n",
    "#df_gwas['STRONGEST SNP-RISK ALLELE'].drop_duplicates()\n",
    "df_gwas[['MAPPED_GENE','DISEASE/TRAIT','MAPPED_TRAIT','SNP_ID_CURRENT','STRONGEST SNP-RISK ALLELE','RISK ALLELE FREQUENCY']].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('SNPs:',         df_gwas.SNPS            .nunique())\n",
    "print('DISEASE/TRAIT:',df_gwas['DISEASE/TRAIT'].nunique())\n",
    "print('MAPPED_TRAIT:', df_gwas['MAPPED_TRAIT'] .nunique())\n",
    "\n",
    "df_gwas['MAPPED_TRAIT'].value_counts()#.to_csv('MAPPED_TRAIT_value_count.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Filter GWAS dataset (required)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "choose_df = clean_df_gwas # or full df_gwas\n",
    "\n",
    "# Drop non risk/effect allele:\n",
    "mask = df_gwas['STRONGEST SNP-RISK ALLELE'].str.contains(\"\\?\")\n",
    "df_gwas_drop = choose_df[-mask].reset_index(drop=True)\n",
    "\n",
    "# Drop complementary base allele (risk allele freq missing)\n",
    "df_gwas_drop_nonan = df_gwas_drop.dropna(subset=['RISK ALLELE FREQUENCY'],axis=0).reset_index(drop=True)\n",
    "\n",
    "print('Drop no risk allele:')\n",
    "print('SNPs:',len(df_gwas_drop_nonan.SNPS.drop_duplicates()))\n",
    "print('DISEASE/TRAIT:',len(df_gwas_drop_nonan['DISEASE/TRAIT'].drop_duplicates()))\n",
    "print('MAPPED_TRAIT:',len(df_gwas_drop_nonan['MAPPED_TRAIT'].drop_duplicates()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Full dataset:\n",
    "SNPs: 267372\n",
    "DISEASE/TRAIT: 21399\n",
    "MAPPED_TRAIT: 7690"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# - Lookup for rsid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LOOKUP FOR SINGLE RSID\n",
    "rsid_mask = df_gwas_drop_nonan['SNPS'].str.contains('rs1421085')\n",
    "df_gwas_drop_nonan_rsid = df_gwas_drop_nonan[rsid_mask]\n",
    "df_gwas_drop_nonan_rsid[['MAPPED_GENE','DISEASE/TRAIT','MAPPED_TRAIT','SNP_ID_CURRENT','STRONGEST SNP-RISK ALLELE','RISK ALLELE FREQUENCY']].drop_duplicates()\n",
    "df_gwas_drop_nonan_rsid.value_counts('DISEASE/TRAIT')\n",
    "df_gwas_drop_nonan_rsid.value_counts('STRONGEST SNP-RISK ALLELE')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display and to clipboard\n",
    "df_gwas_drop_nonan[['MAPPED_GENE', 'DISEASE/TRAIT', 'SNPS','STRONGEST SNP-RISK ALLELE', 'RISK ALLELE FREQUENCY']].drop_duplicates()#to_clipboard()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_gwas_drop_nonan_rsid[['MAPPED_GENE', 'DISEASE/TRAIT', 'SNPS','STRONGEST SNP-RISK ALLELE', 'RISK ALLELE FREQUENCY']].drop_duplicates()#.to_clipboard()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Merge GWAS and GRPMX data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. choose and load GRPM survey to merge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# choose db\n",
    "db_tag = 'pcg'\n",
    "# pcg    = protein coding genes = grpm_db_pcg\n",
    "# rna    = rna genes            = grpm_db_rna\n",
    "# pseudo = pseudogenes          = grpm_db_pseudo\n",
    "#-------------------------------------------------\n",
    "\n",
    "survey_path = 'grpm_surveys/'\n",
    "\n",
    "# Create an empty list to store folder names\n",
    "folder_names = []\n",
    "current_dir = os.getcwd()+'/'+survey_path\n",
    "# Iterate over the directories in the workspace\n",
    "for root, dirs, files in os.walk(current_dir):\n",
    "    for dir_name in dirs:\n",
    "        # Check if the folder name contains the string 'survey'\n",
    "        if 'survey' in dir_name:\n",
    "            folder_names.append(dir_name)\n",
    "\n",
    "# Create a pandas Series from the list of folder names\n",
    "folder_series = pd.Series(folder_names)\n",
    "print('Available survey repositories:\\n')\n",
    "folder_series = folder_series.str.replace('grpm_survey_'+db_tag+'_','')\n",
    "# Print the resulting Series\n",
    "print(folder_series)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load my GRPMx Data from survey folder\n",
    "tag = 'nutri'\n",
    "directory = survey_path+'grpm_survey_pcg_'+tag\n",
    "df_grpmx = pd.read_csv(directory+'/grpmx_filtered_output.csv', index_col=0)\n",
    "df_grpmx_repo = pd.read_csv(directory+'/GRPMX_report_int.csv')\n",
    "\n",
    "# add mesh synonyms\n",
    "mesh_df = pd.read_csv('ref-mesh-archive/MESH_STY_LITVAR1.csv')[['Preferred Label', 'Synonyms']]\n",
    "df_grpmx = pd.merge(df_grpmx,\n",
    "                            mesh_df, left_on='mesh',right_on='Preferred Label')\n",
    "mesh_df = None\n",
    "\n",
    "df_grpmx = df_grpmx.drop('Preferred Label', axis = 1)\n",
    "df_grpmx['all_mesh'] = df_grpmx['mesh'] + ', ' + df_grpmx['Synonyms']\n",
    "\n",
    "\n",
    "#def function: filter for int threshold:\n",
    "def filter_int(df_repo, threshold ):\n",
    "    df_grpmx_repo_int = df_repo[df_repo.interest_index >= threshold]\n",
    "    return df_grpmx[df_grpmx.gene.isin(df_grpmx_repo_int.gene)]\n",
    "\n",
    "#filter for 0.95 quantile\n",
    "df_grpmx_95 = filter_int(df_grpmx_repo, threshold=df_grpmx_repo.interest_index.quantile(0.95))\n",
    "\n",
    "#filter for int threshold:\n",
    "df_grpmx_int = filter_int(df_grpmx_repo, threshold=0.0125)\n",
    "\n",
    "print('df_grpmx_95 genes:', df_grpmx_95.gene.nunique())\n",
    "print('df_grpmx_int genes:', df_grpmx_int.gene.nunique())\n",
    "\n",
    "df_grpmx_int"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. set Gene Interest threshold"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#threshold\n",
    "#df_grpmx_th = df_grpmx_95\n",
    "df_grpmx_th = df_grpmx_int\n",
    "\n",
    "df_grpmx_int = pd.DataFrame()\n",
    "df_grpmx_th_int = pd.DataFrame()\n",
    "\n",
    "## ADD gene-interest index as common sorting handle\n",
    "small_dummy = df_grpmx_repo[['gene','interest_index']]\n",
    "df_grpmx_int =    pd.merge(df_grpmx,    small_dummy, left_on='gene', right_on='gene')\n",
    "df_grpmx_th_int = pd.merge(df_grpmx_th, small_dummy, left_on='gene', right_on='gene')\n",
    "\n",
    "print('GRPMX threshold Statistics:')\n",
    "print('genes:', df_grpmx_th.gene.nunique())\n",
    "print('rsid:',  df_grpmx_th .rsid.nunique())\n",
    "print('mesh:',  df_grpmx_th .mesh.nunique())\n",
    "\n",
    "df_grpmx_int\n",
    "df_grpmx_th_int"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. merge gwas_df with grpmx dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - complete grpmx merge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_with = clean_df_gwas # or full df_gwas\n",
    "\n",
    "#------------------------------------------\n",
    "\n",
    "#merge_with = df_gwas # or full df_gwas\n",
    "def typestr(df):\n",
    "    df[['pmids','PUBMEDID']] = df[['pmids','PUBMEDID']].astype(str)\n",
    "\n",
    "# sort grpmx geeens by interest index:\n",
    "\n",
    "#common handle sort\n",
    "df_grpmx_int = df_grpmx_int.sort_values(by=['interest_index','rsid','mesh'], ascending =False).reset_index(drop=True)\n",
    "\n",
    "timea = datetime.now()\n",
    "print('merging data, please wait... ')\n",
    "# Merge two df on rsid:\n",
    "df_merged = pd.merge(df_grpmx_int,\n",
    "                     merge_with, left_on='rsid', right_on='SNPS')\n",
    "typestr(df_merged)\n",
    "\n",
    "df_merged_drop = pd.merge(df_grpmx_int,\n",
    "                          df_gwas_drop_nonan, left_on='rsid', right_on='SNPS')\n",
    "typestr(df_merged_drop)\n",
    "\n",
    "#rename columns:\n",
    "def rename_col(df):\n",
    "    return df.rename(columns={'gene':'LITVAR_GENE', 'rsid':'LIVAR_RSID', 'pmids':'LITVAR_PMID','mesh':'PUBMED_MESH'}, inplace=True)\n",
    "rename_col(df_merged)\n",
    "rename_col(df_merged_drop)\n",
    "\n",
    "print('runtime:', datetime.now()-timea)\n",
    "df_merged_drop[['LITVAR_GENE','MAPPED_GENE','PUBMED_MESH','all_mesh','DISEASE/TRAIT','MAPPED_TRAIT']].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#stats\n",
    "print('Complete grpmx merge Stats\\n')\n",
    "\n",
    "print('grpmx_gene',df_grpmx_int.gene.nunique())\n",
    "print('grpmx_mesh',df_grpmx_int.mesh.nunique())\n",
    "print('grpmx_rsid',df_grpmx_int.rsid.nunique())\n",
    "\n",
    "#print('\\nnonan',len(df_merged_drop),', full:', len(df_merged))\n",
    "df_merged_drop[['LITVAR_GENE','LIVAR_RSID','DISEASE/TRAIT','STRONGEST SNP-RISK ALLELE']].drop_duplicates()\n",
    "print('df merged:')\n",
    "print(df_merged_drop[['LITVAR_GENE','LITVAR_PMID','PUBMED_MESH','LIVAR_RSID','DISEASE/TRAIT','STRONGEST SNP-RISK ALLELE']].nunique())\n",
    "\n",
    "df_merged_drop[['LITVAR_GENE','LIVAR_RSID','PUBMED_MESH','DISEASE/TRAIT','STRONGEST SNP-RISK ALLELE']].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - threshold grpmx merge\n",
    "(it's better top apply threshold downstream, skip this)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# threshold merge on SNPs\n",
    "merge_also_nonan = True\n",
    "\n",
    "def typestr(df):\n",
    "    df[['pmids','PUBMEDID']] = df[['pmids','PUBMEDID']].astype(str)\n",
    "\n",
    "df_grpmx_th_int = df_grpmx_th_int.sort_values(by=['interest_index','rsid','mesh'], ascending =False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Merge two df on rsid:\n",
    "if merge_also_nonan == False:\n",
    "    df_merged_th = pd.merge(df_grpmx_th_int,\n",
    "                            df_gwas, left_on='rsid', right_on='SNPS')\n",
    "    typestr(df_merged_th)\n",
    "else:\n",
    "    df_merged_th = pd.merge(df_grpmx_th_int,\n",
    "                            df_gwas, left_on='rsid', right_on='SNPS')\n",
    "    typestr(df_merged_th)\n",
    "\n",
    "    df_merged_th_drop = pd.merge(df_grpmx_th_int,\n",
    "                                 df_gwas_drop_nonan, left_on='rsid', right_on='SNPS')\n",
    "    typestr(df_merged_th_drop)\n",
    "\n",
    "#rename columns:\n",
    "def rename_col(df):\n",
    "    return df.rename(columns={'gene':'LITVAR_GENE', 'rsid':'LIVAR_RSID', 'pmids':'LITVAR_PMID','mesh':'PUBMED_MESH'}, inplace=True)\n",
    "rename_col(df_merged_th_drop)\n",
    "rename_col(df_merged_th)\n",
    "\n",
    "print('genes merged:', df_merged_th_drop.LITVAR_GENE.nunique())\n",
    "df_merged_th_drop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#stats\n",
    "print('Threshold grpmx merge Statistics:\\n')\n",
    "print('grpmx_gene',df_grpmx_th_int.gene.nunique())\n",
    "print('grpmx_mesh',df_grpmx_th_int.mesh.nunique())\n",
    "print('grpmx_rsid',df_grpmx_th_int.rsid.nunique())\n",
    "\n",
    "print('\\nfull threshold merge pmids:',df_merged_th.LITVAR_PMID.nunique(),\n",
    "      'threshold pmids:',df_merged_th_drop.LITVAR_PMID.nunique())\n",
    "#df_merged_95_drop[['gene','rsid','mesh','DISEASE/TRAIT','STRONGEST SNP-RISK ALLELE']].drop_duplicates()\n",
    "print('\\ngrpm merged with gwas stats:')\n",
    "print(df_merged_th_drop[['LITVAR_GENE','LITVAR_PMID','PUBMED_MESH','LIVAR_RSID','DISEASE/TRAIT','STRONGEST SNP-RISK ALLELE']].nunique())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_unique_values(df):\n",
    "    unique_counts = df.nunique()  # Calculate the number of unique values for each column\n",
    "    plt.figure(figsize=(8,3))\n",
    "    unique_counts.plot(kind='bar')  # Create a bar plot\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Unique Values')\n",
    "    plt.title('Number of Unique Values per Column')\n",
    "    plt.show()\n",
    "visualize_unique_values(df_merged_th_drop)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# complete grpmx merge stats:\n",
    "\n",
    "# value counts:\n",
    "print('GWAS TRAIT count in entire GRPM survey')\n",
    "df_merged.value_counts('DISEASE/TRAIT')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('GWAS TRAIT count (nonan) in entire grpmx')\n",
    "df_merged_drop.value_counts('DISEASE/TRAIT')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df_merged_drop_nonan.to_csv('gwas_catalog_data/df_merged_drop_nonan.csv') # heavy file!\n",
    "def df_usage(df):\n",
    "    return (df.memory_usage()/1048576).sum()\n",
    "\n",
    "print(df_usage(df_merged))\n",
    "print(df_usage(df_merged_th_drop))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Filter for lexical match"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - create correspondence GWAS-GRPM df\n",
    "['PUBMED_MESH','DISEASE/TRAIT']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(df_merged_drop[['LITVAR_GENE','MAPPED_GENE']].nunique())\n",
    "print('')\n",
    "# choosing 'DISEASE/TRAIT' or 'MAPPED_TRAIT'\n",
    "print(df_merged_drop[['DISEASE/TRAIT','MAPPED_TRAIT','PUBMED_MESH' ]].nunique())\n",
    "df_merged_drop[['DISEASE/TRAIT','MAPPED_TRAIT' ]].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create correspondence GWAS-GRPM df\n",
    "mesh_col = 'all_mesh'\n",
    "corr_df_all = df_merged_drop[[mesh_col,'MAPPED_TRAIT' ]].drop_duplicates().reset_index(drop= True).dropna()\n",
    "print('PUBMED_MESH',   corr_df_all[mesh_col].nunique())\n",
    "print('MAPPED_TRAIT' , corr_df_all['MAPPED_TRAIT' ].nunique())\n",
    "print('rows' ,     len(corr_df_all))\n",
    "#print(corr_df.sort_values(by= mesh_col))\n",
    "\n",
    "mesh_col = 'PUBMED_MESH'\n",
    "corr_df = df_merged_drop[[mesh_col,'MAPPED_TRAIT' ]].drop_duplicates().reset_index(drop= True)\n",
    "print('\\nPUBMED_MESH',   corr_df[mesh_col].nunique())\n",
    "print(  'MAPPED_TRAIT' , corr_df['MAPPED_TRAIT' ].nunique())\n",
    "print(  'rows' ,         len(corr_df))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_df.sort_values(by= 'PUBMED_MESH')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#corr_df[mesh_col].drop_duplicates()\n",
    "df_merged_drop\n",
    "#mesh  363"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - build a common Dictionary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mesh_col = 'PUBMED_MESH'\n",
    "meshes = corr_df.sort_values(by= mesh_col)[mesh_col].drop_duplicates()\n",
    "mesh_corr_df = corr_df[corr_df[mesh_col] == meshes.iloc[100]]\n",
    "\n",
    "# Correspondence through AI  (trial)\n",
    "if simple_bool('Try Correspondence through AI?'):\n",
    "    import pychatgpt as op\n",
    "    mess = \"analyze and filter the csv below, creating another csv keeping only the rows where the 'PUBMED_MESH' and 'MAPPED_TRAIT' are the exact same biological entity even though it could be written differently:\\n\\n\"+mesh_corr_df.to_csv(index=None)\n",
    "    op.ask_gpt(mess)#, maxtoken=1500)\n",
    "    mesh_corr_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - through Tokenization (nltk)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Correspondence dictionary through Tokenization:\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# choose df:\n",
    "df = corr_df_all\n",
    "mesh_col = 'all_mesh'\n",
    "\n",
    "if False: #(pseudo-code)\n",
    "    # Function to tokenize a string into individual words\n",
    "    def tokenize_string(text):\n",
    "        return set(word_tokenize(text.lower()))\n",
    "\n",
    "    # Filter the DataFrame based on the condition that the intersection of tokenized \"PUBMED MESH\" and \"MAPPED_TRAIT\" is not empty\n",
    "    filtered_df = df[df.apply(lambda row: bool(tokenize_string(row[mesh_col]) & tokenize_string(row['MAPPED_TRAIT'])), axis=1)]\n",
    "\n",
    "    tokenize_string(corr_df[mesh_col][2])\n",
    "    n =55\n",
    "    bool(tokenize_string(corr_df[mesh_col][n]) & tokenize_string(corr_df['MAPPED_TRAIT'][n]))\n",
    "    print(tokenize_string_trial(corr_df[mesh_col][n]))\n",
    "    print(tokenize_string(corr_df[mesh_col][n]))\n",
    "#-------------\n",
    "\n",
    "print('tokenization in progress, please wait...')\n",
    "# choosen token TAGs:\n",
    "def tokenize_string_trial(text):\n",
    "    tokens = set(word_tokenize(text.lower()))\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    desired_tags = ['NN', 'NNS', 'JJ']\n",
    "    filtered_tokens = [token for token, pos in tagged_tokens if pos in desired_tags]\n",
    "    return set(filtered_tokens)\n",
    "\n",
    "timea = datetime.now()\n",
    "filtered_df = df[df.apply(lambda row: bool(tokenize_string_trial(row[mesh_col]) & tokenize_string_trial(row['MAPPED_TRAIT'])), axis=1)]\n",
    "timeb= datetime.now()\n",
    "print('runtime:', timeb-timea)\n",
    "\n",
    "filtered_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. merging Dictionay to GWAS_GRPM_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_merged_drop.all_mesh\n",
    "filtered_df.all_mesh"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# filtering merge:\n",
    "mesh_col = 'all_mesh'\n",
    "merge_grpm_gwas_fliter = df_merged_drop.merge(filtered_df, on=[mesh_col,'MAPPED_TRAIT'])\n",
    "df_show = merge_grpm_gwas_fliter[['LITVAR_GENE','LIVAR_RSID', 'LITVAR_PMID', 'PUBMED_MESH', 'Synonyms','interest_index','MAPPED_GENE','PUBMED_MESH','DISEASE/TRAIT','MAPPED_TRAIT', 'STRONGEST SNP-RISK ALLELE','P-VALUE', 'OR or BETA']].drop_duplicates().reset_index(drop= True)\n",
    "df_show = df_show.loc[:, ~df_show.columns.duplicated()]\n",
    "df_show"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_show.to_csv('gwas_catalog_data/merge_grpm_gwas_fliter_nutri_0725.csv')\n",
    "pd.read_csv('gwas_catalog_data/merge_grpm_gwas_fliter_nutri_0725.csv')\n",
    "\n",
    "#statistics\n",
    "print(df_show[['LITVAR_GENE','LIVAR_RSID','LITVAR_PMID','PUBMED_MESH','MAPPED_TRAIT','DISEASE/TRAIT','STRONGEST SNP-RISK ALLELE']].nunique())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### apply GI threshold"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# filtering fot threshold GI:\n",
    "threshold = 0.0125\n",
    "df_show_th = df_show[df_show.interest_index >= threshold]\n",
    "\n",
    "df_show_th.to_csv('gwas_catalog_data/merge_grpm_gwas_fliter_nutri_th0136.csv')\n",
    "print('merged GWAS-GRPMX threshold:', threshold,'\\n')\n",
    "print(df_show_th[['LITVAR_GENE','MAPPED_GENE','PUBMED_MESH','MAPPED_TRAIT','DISEASE/TRAIT','STRONGEST SNP-RISK ALLELE']].nunique())\n",
    "\n",
    "df_thr_short= df_show_th[['LITVAR_GENE','LIVAR_RSID','LITVAR_PMID','PUBMED_MESH','MAPPED_TRAIT','DISEASE/TRAIT','STRONGEST SNP-RISK ALLELE']]\n",
    "df_thr_short#.to_csv(r'file_name.csv')\n",
    "df_show_th.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_thr_short.drop_duplicates(subset='LITVAR_GENE').sample(frac=1)#.to_csv('file_name.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_grpmx_int#.gene.nunuque()\n",
    "df_merged_drop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Scoping GWAS Dataset (general)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Mother Dataframe:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_merged_drop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LOOKUP FOR SINGLE RSID\n",
    "rsid_mask = df_merged_drop_nonan['LIVAR RSID'].str.contains('rs1421085')\n",
    "df_merged_drop_nonan_rsid = df_merged_drop_nonan[rsid_mask]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display and to clipboard\n",
    "df_merged_drop[['LITVAR_GENE','LIVAR RSID','MAPPED_GENE','PUBMED_MESH', 'DISEASE/TRAIT', 'STRONGEST SNP-RISK ALLELE', 'RISK ALLELE FREQUENCY']].drop_duplicates()\n",
    "df_merged_drop_nonan_rsid[['LITVAR_GENE','LIVAR RSID','MAPPED_GENE','PUBMED_MESH', 'DISEASE/TRAIT', 'STRONGEST SNP-RISK ALLELE', 'RISK ALLELE FREQUENCY']].drop_duplicates()#.to_clipboard()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_merged_drop_nonan_rsid['PUBMED_MESH'].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get risk Allele list and use it to filter mother table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_merged_drop_nonan.value_counts('SNPS')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df_merged_drop_rsid_nonan[['STRONGEST SNP-RISK ALLELE','PUBMED_MESH']].groupby('STRONGEST SNP-RISK ALLELE').describe().reset_index()\n",
    "rsid_mask = df_merged_drop_nonan['LIVAR RSID'].str.contains('rs1421085')\n",
    "df_merged_drop_nonan_rsid = df_merged_drop_nonan[rsid_mask]\n",
    "df_merged_drop_nonan_rsid[['LITVAR_GENE','LIVAR RSID','MAPPED_GENE','PUBMED_MESH', 'DISEASE/TRAIT', 'STRONGEST SNP-RISK ALLELE', 'RISK ALLELE FREQUENCY']].drop_duplicates()\n",
    "type(df_merged_drop_nonan_rsid['STRONGEST SNP-RISK ALLELE'].value_counts())#.head(1))\n",
    "risk_allele = df_merged_drop_nonan_rsid['STRONGEST SNP-RISK ALLELE'].value_counts()#.index[0]\n",
    "risk_allele"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#---> creare una lista programmaticamente di tutti i 'risk allele' by count and use it to filter mother dataframe with isin module!\n",
    "\n",
    "# get all rsid list\n",
    "rsid_list = df_merged_drop_nonan['LIVAR RSID'].drop_duplicates().to_list()\n",
    "len(rsid_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# risk allele pickup (part1)\n",
    "time_start = datetime.now()\n",
    "risk_allele_list = []\n",
    "for i in rsid_list[:2000]:\n",
    "    rsid_mask = df_merged_drop_nonan['LIVAR RSID'].str.contains(i)\n",
    "    df_merged_drop_nonan_rsid = df_merged_drop_nonan[rsid_mask]\n",
    "    risk_allele = df_merged_drop_nonan_rsid['STRONGEST SNP-RISK ALLELE'].value_counts().index[0]\n",
    "    risk_allele_list.append(risk_allele)\n",
    "    #print(str(risk_allele))\n",
    "finish_start = datetime.now()\n",
    "pd.Series(risk_allele_list).to_csv('gwas_catalog_data/risk_allele_list_0-2000.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# risk allele pickup (part2)\n",
    "for i in rsid_list[2000:]:\n",
    "    rsid_mask = df_merged_drop_nonan['LIVAR RSID'].str.contains(i)\n",
    "    df_merged_drop_nonan_rsid = df_merged_drop_nonan[rsid_mask]\n",
    "    risk_allele = df_merged_drop_nonan_rsid['STRONGEST SNP-RISK ALLELE'].value_counts().index[0]\n",
    "    risk_allele_list.append(risk_allele)\n",
    "    #print(str(risk_allele))\n",
    "finish_start = datetime.now()\n",
    "print(finish_start - time_start)\n",
    "pd.Series(risk_allele_list).to_csv('gwas_catalog_data/risk_allele_list_0-2000.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import back risk allele list\n",
    "\n",
    "risk_allele_df = pd.read_csv('gwas_catalog_data/risk_allele_list_4376.csv', index_col=0)\n",
    "risk_allele_list = risk_allele_df['0'].to_list()\n",
    "risk_allele_list\n",
    "# --> ora filtrare Mother Df per i risk allele and ...BAM!\n",
    "risk_allele_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# genera la merged  with GRPMX dropped!\n",
    "#df_merged_drop_nonan = pd.read_csv('df_merged_drop_nonan.csv', index_col=0)\n",
    "\n",
    "df_merged_drop_less_gwa = df_merged_drop_nonan[['MAPPED_GENE','SNPS', 'DISEASE/TRAIT', 'STRONGEST SNP-RISK ALLELE', 'RISK ALLELE FREQUENCY']].drop_duplicates()\n",
    "#df_merged_drop_less_gwa.to_csv('df_merged_drop_less_gwa.csv')#.SNPS.drop_duplicates()\n",
    "df_merged_drop_less_gwa.SNPS.drop_duplicates().sample(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type(df_merged_drop_nonan_rsid['STRONGEST SNP-RISK ALLELE'].value_counts())#.head(1))\n",
    "risk_allele = df_merged_drop_nonan_rsid['STRONGEST SNP-RISK ALLELE'].value_counts()#.index[0]\n",
    "risk_allele"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#qualti sono monorischio??\n",
    "rsid_list = df_merged_drop_less_gwa.SNPS.drop_duplicates().to_list()\n",
    "time_start = datetime.now()\n",
    "monorisk_list = []\n",
    "\n",
    "for i in rsid_list:\n",
    "    rsid_mask = df_merged_drop_less_gwa['SNPS'].str.contains(i)\n",
    "    df_merged_drop_less_gwa_rsid = df_merged_drop_less_gwa[rsid_mask].drop_duplicates()\n",
    "    risk_allele = df_merged_drop_less_gwa_rsid['STRONGEST SNP-RISK ALLELE'].value_counts()#.index[0]\n",
    "    if len(risk_allele)==1:\n",
    "            monorisk = risk_allele.index[0]\n",
    "            monorisk_list.append(monorisk)\n",
    "            print(risk_allele)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#print(risk_allele)\n",
    "pd.Series(monorisk_list)#.to_csv('gwas_catalog_data/monorisk_list.csv')\n",
    "#len(rsid_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# risk allele pickup: ricerca dei valori di conteggio ambigui------------------\n",
    "\n",
    "rsid_list = df_merged_drop_less_gwa.SNPS.drop_duplicates().to_list()\n",
    "time_start = datetime.now()\n",
    "ambiguity_list = []\n",
    "\n",
    "for i in rsid_list:\n",
    "    rsid_mask = df_merged_drop_less_gwa['SNPS'].str.contains(i)\n",
    "    df_merged_drop_less_gwa_rsid = df_merged_drop_less_gwa[rsid_mask].drop_duplicates()\n",
    "    risk_allele = df_merged_drop_less_gwa_rsid['STRONGEST SNP-RISK ALLELE'].value_counts()#.index[0]\n",
    "    if len(risk_allele)>1:\n",
    "        if risk_allele[0] == risk_allele[1]:\n",
    "            ambiguity = risk_allele.index[0]\n",
    "            ambiguity_list.append(ambiguity)\n",
    "            print(risk_allele)\n",
    "\n",
    "print(risk_allele)\n",
    "ambiguity_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ambiguity_list = []\n",
    "for i in rsid_list:\n",
    "    rsid_mask = df_merged_drop_less_gwa['SNPS'].str.contains(i)\n",
    "    df_merged_drop_less_gwa_rsid = df_merged_drop_less_gwa[rsid_mask].drop_duplicates()\n",
    "    risk_allele = df_merged_drop_less_gwa_rsid['STRONGEST SNP-RISK ALLELE'].value_counts()#.index[0]\n",
    "    if len(risk_allele)>1:\n",
    "        if risk_allele[0] == risk_allele[1]:\n",
    "            ambiguity = risk_allele.index[0], risk_allele[0]\n",
    "            ambiguity_list.append(ambiguity)\n",
    "            #print(risk_allele)\n",
    "ambiguity_df = pd.DataFrame(ambiguity_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#ambiguity_df.to_csv('gwas_catalog_data/ambiguity_magg1_df.csv')\n",
    "pd.read_csv('gwas_catalog_data/ambiguity_1_list.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ambiguity_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ambiguity_df.groupby(by=1).describe().to_clipboard()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## remove ambiguity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "risk_allele_df#[0]\n",
    "df_merged_drop_less_gwa\n",
    "type(risk_allele_df.iloc[:,0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select just ambiugous count > 1\n",
    "ambiguous_magg1 = pd.read_csv('gwas_catalog_data/ambiguity_magg1_df.csv', index_col=0)\n",
    "# Only GWAS\n",
    "mask = df_merged_drop_less_gwa['SNPS'].isin(ambiguous_magg1.rsid)\n",
    "df_merged_drop_less_gwa_ambmagg1 = df_merged_drop_less_gwa[mask]\n",
    "df_merged_drop_less_gwa_ambmagg1#.to_csv('gwas_catalog_data/df_merged_drop_less_gwa_ambmagg1.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#GRPMX-GWAS\n",
    "mask = df_merged_drop_nonan['SNPS'].isin(ambiguous_magg1.rsid)\n",
    "df_merged_drop_nonan_ambmagg1 = df_merged_drop_nonan[mask]\n",
    "df_merged_drop_nonan_ambmagg1[['LITVAR_GENE','PUBMED_MESH','DISEASE/TRAIT','STRONGEST SNP-RISK ALLELE','RISK ALLELE FREQUENCY','MAPPED_GENE']]#.to_csv('gwas_catalog_data/df_merged_drop_less_gwa_ambmagg1.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_merged_drop_nonan[['LITVAR_GENE','PUBMED_MESH','DISEASE/TRAIT','STRONGEST SNP-RISK ALLELE','RISK ALLELE FREQUENCY','MAPPED_GENE']]\n",
    "#quanti sono monorisk allele?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# before filter mother df with \"riskallelelist\n",
    "\n",
    "# Only GWAS\n",
    "mask = df_merged_drop_less_gwa['STRONGEST SNP-RISK ALLELE'].isin(risk_allele_df.iloc[:,0])\n",
    "df_merged_drop_less_gwa_riskall = df_merged_drop_less_gwa[mask]\n",
    "#df_merged_drop_less_gwa_riskall.to_csv('gwas_catalog_data/df_merged_drop_less_gwa_riskall.csv')\n",
    "df_merged_drop_less_gwa_riskall"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GRPMX-GWAS merged\n",
    "mask2 = df_merged_drop_nonan['STRONGEST SNP-RISK ALLELE'].isin(risk_allele_df.iloc[:,0])\n",
    "df_merged_drop_nonan_riskall = df_merged_drop_nonan[mask2]\n",
    "df_merged_drop_nonan_riskall.to_csv('gwas_catalog_data/df_merged_drop_nonan_riskall.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# then remove ambiugous rsids\n",
    "\n",
    "# GRPMX-GWAS merged\n",
    "ambiguous_rsids = pd.read_csv('gwas_catalog_data/ambiguous_rsids.csv')\n",
    "mask_amb = df_merged_drop_nonan_riskall.SNPS.isin(ambiguous_rsids['ambiguous_rsids'])\n",
    "df_merged_drop_nonan_riskall_unamb = df_merged_drop_nonan_riskall[-mask_amb]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_merged_drop_nonan_riskall_unamb.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_merged_drop_nonan_riskall_unamb[['LITVAR_GENE', 'LIVAR RSID', 'LITVAR PMID', 'PUBMED_MESH', 'PUBMEDID', 'DISEASE/TRAIT','MAPPED_GENE','STRONGEST SNP-RISK ALLELE', 'SNPS','RISK ALLELE FREQUENCY', 'P-VALUE','OR or BETA']][300:325].to_clipboard(sep=',')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Only GWAS\n",
    "mask_amb = df_merged_drop_less_gwa_riskall.SNPS.isin(ambiguous_rsids['ambiguous_rsids'])\n",
    "df_merged_drop_less_gwa_riskall_unamb = df_merged_drop_less_gwa_riskall[-mask_amb]\n",
    "df_merged_drop_less_gwa_riskall_unamb.to_csv('gwas_catalog_data/df_merged_drop_less_gwa_unambiguous.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_merged_drop_less_gwa_riskall_unamb[300:325].to_clipboard(sep=',')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(df_merged_drop_less_gwa_riskall_unamb.SNPS.drop_duplicates()), len(df_merged_drop_nonan.SNPS.drop_duplicates())\n",
    "unambiguous_rsids = df_merged_drop_less_gwa_riskall_unamb.SNPS.drop_duplicates().reset_index(drop=True)\n",
    "#unambiguous_rsids.to_csv('gwas_catalog_data/unambiguous_rsids.csv')\n",
    "#df_merged_drop_nonan_unamb.to_csv('gwas_catalog_data/df_merged_drop_nonan_unambiguous.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_merged_drop_less_gwa_unamb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LOOKUP FOR SINGLE RSID\n",
    "rsid_mask = df_merged_drop_less_gwa['SNPS'].str.contains('rs1558902')\n",
    "df_merged_drop_less_gwa_rsid = df_merged_drop_less_gwa[rsid_mask]\n",
    "df_merged_drop_less_gwa_rsid.to_clipboard()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LOOKUP FOR SINGLE ALLELE\n",
    "allele_mask = df_merged_drop_nonan_rsid['STRONGEST SNP-RISK ALLELE'].str.contains('-C')\n",
    "df_merged_drop_rsid_allele = df_merged_drop_nonan_rsid[allele_mask]\n",
    "df_merged_drop_rsid_allele[['LITVAR_GENE','LIVAR RSID','MAPPED_GENE','PUBMED_MESH', 'DISEASE/TRAIT', 'STRONGEST SNP-RISK ALLELE', 'RISK ALLELE FREQUENCY']].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trials"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizing (Trial)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tokenizing (Trial)\n",
    "import nltk\n",
    "def tokenize_string_trial(text):\n",
    "    tokens = set(word_tokenize(text.lower()))\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    desired_tags = ['NN', 'NNS', 'JJ','VB']\n",
    "    filtered_tokens = [token for token, pos in tagged_tokens if pos in desired_tags]\n",
    "    return set(filtered_tokens)\n",
    "\n",
    "sentence = \"\"\"The adipokines, or adipocytokines (Greek adipo-, fat; cytos-, cell; and -kinos, movement) are cytokines (cell signaling proteins) secreted by adipose tissue. Some contribute to an obesity-related low-grade state of inflammation or to the development of metabolic syndrome, a constellation of diseases including, but not limited to, type 2 diabetes, cardiovascular disease and atherosclerosis.[1] The first adipokine to be discovered was leptin in 1994.[2] Since that time, hundreds of adipokines have been discovered.[3]\"\"\"\n",
    "tokens = nltk.word_tokenize(corr_df['PUBMED_MESH'][5].lower())\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "tagg= pd.DataFrame(tagged[0:])\n",
    "\n",
    "#print(tagg.groupby(by=1 ).describe())\n",
    "tagg[tagg[1]== 'VBD']\n",
    "print(type(tagged))\n",
    "\n",
    "print('\\n',tokenize_string_trial(sentence))\n",
    "print('\\n',tokenize_string(sentence))\n",
    "#tagg[1].drop_duplicates().to_list()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!pip install svgling\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "type(entities)\n",
    "entities"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
